#######################################################################################################################
# General tuned guide & details about the configuration:
# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/chap-red_hat_enterprise_linux-performance_tuning_guide-tuned
#
# This profile optimizes a RHEL or RHEL-based (e.g CentOS) to perform best with splunk
#
# Usually there is no need to adjust settings here but some requires a manual setup.
# Checkout the following stanzas and adapting them to your server setup:
#  * script_SSDs (check disk names within the script)
#  * disk_scheduler (check devices key)
#  * sysctl (write cache topic - if it fits your RAM)
#######################################################################################################################
[main]
# latency-performance ensures we have all the required settings in place. do not use virtual-guest here
# if you prefer any other profile ensure all settings like swap handling is set manually according to 
# the recommended profile "latency-performance"
include=latency-performance
# ensure the defaults will not overwrite anything (usually not needed when defined at the top but to be double save..)
replace=false

[vm]
# this should be set by latency-performance but we want to ensure that this is set always
# https://access.redhat.com/solutions/1265183
# https://docs.splunk.com/Documentation/Splunk/latest/ReleaseNotes/SplunkandTHP
transparent_hugepages=never

[script defrag]
type = script
# disable THP defrag
script = never_defrag.sh

#[script_SSDs]
#type = script
# this optimizes flash cards and ssd's - MANUAL ACTION REQUIRED (devices must be defined in tune_ssd.sh)
#script = tune_ssd.sh

[script_HDDs]
type = script
# this optimizes rotating disks - fully automated (i.e. no devices must be defined)
script = tune_hdd.sh

[disk_scheduler]
type = disk
# as long as we run in a VM using an I/O scheduler is contra productive: https://access.redhat.com/solutions/5427
# more details on the available I/O schedulers:
# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/chap-red_hat_enterprise_linux-performance_tuning_guide-storage_and_file_systems#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Considerations-IO_Schedulers
devices = [vs]d*
elevator = noop

[sysctl]
# fix: "kernel: TCP: request_sock_TCP: Possible SYN flooding on port 8089. Sending cookies."
# https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=19f92a030ca6d772ab44b22ee6a01378a8cb32d4
# and https://access.redhat.com/solutions/30453#Valid
net.core.somaxconn = 4096
# Workaround for SPL-99110 (non-public splunk "known issue")
# increase the maximum socket receive buffer size to 128 MB
net.core.rmem_max = 134217728
# increase the maximum socket send buffer size to 128 MB
net.core.wmem_max = 134217728
# TCP buffer limits min, default, and max number of bytes
net.ipv4.tcp_rmem = 16384 87380 67108864
net.ipv4.tcp_wmem = 16384 87380 67108864

#### write cache
# Details: 
#  * https://www.suse.com/support/kb/doc/?id=000017857
#  * https://helpful.knobs-dials.com/index.php/Computer_data_storage_-_General_%26_RAID_performance_tweaking#Tweaking
#
# These settings are needed on high RAM systems like splunk indexers
#
# When this amount of memory is hit, processes will not be allowed to write more until some of their cached data is 
# written out. This ensures that the limit is enforced. By itself, that can slow down writes noticeably, but not 
# tremendously. However, if an application has written a large amount of data which is still in the dirty cache, and 
# then issues a "sync" command to have it all written to disk, this can take a significant amount of time to accomplish.
# During that time, some applications may appear stuck or hung. Some applications which have timers watching those
# processes may even believe that too much time has passed and the operation needs to be aborted, also known as a "timeout".
# set the max dirty cache to 2 GB (disables vm.dirty_ratio)
vm.dirty_bytes = 2147483648
# even though the above will set the following to 0 anyways we have to set them explicitly here to avoid issues with
# tuned-adm verify:
vm.dirty_ratio = 0
# set the background sync to start at 500 MB (disables vm.dirty_background_ratio)
# "Background" writes are kicked off to get writing done even when the application isn't forcing a sync, and even if 
# the dirty_ratio has not yet been reached.   The goal of this setting is to keeps the dirty cache from growing too large.
# Rule of thumb: dirty_background_bytes/ratio = 1/4 to 1/2 of the dirty_bytes/ratio.
vm.dirty_background_bytes = 524288000
# even though the above will set the following to 0 anyways we have to set them explicitly here to avoid issues with
# tuned-adm verify:
vm.dirty_background_ratio = 0

# obsolete because of latency-performance profile
# details: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Performance_Tuning_Guide/s-memory-tunables.html
# https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Performance_Tuning_Guide/sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_tools-Configuring_system_memory_capacity.html#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuring_system_memory_capacity-Virtual_Memory_parameters
#vm.swappiness = 10
